# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctRuxX_CdqC8cqlEZX0lOwL-GqEDQq_q
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class CVNetPreprocessor:
    """
    Advanced preprocessing class for sensor data in CV-Net model
    """
    def __init__(self, sequence_length=50, test_size=0.2, val_size=0.125, random_state=42):
        """
        Initialize the preprocessor with configuration parameters

        Args:
            sequence_length (int): Length of sequences for time series data
            test_size (float): Proportion of data to use for testing
            val_size (float): Proportion of training data to use for validation
            random_state (int): Random seed for reproducibility
        """
        self.sequence_length = sequence_length
        self.test_size = test_size
        self.val_size = val_size
        self.random_state = random_state
        self.standard_scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.minmax_scaler = MinMaxScaler()
        self.knn_imputer = KNNImputer(n_neighbors=5)

    def load_and_combine_data(self, file_paths, verbose=True):
        """
        Load and combine multiple data files

        Args:
            file_paths (list): List of file paths to load
            verbose (bool): Whether to print information during processing

        Returns:
            combined_df (DataFrame): Combined dataframe with all data
        """
        dataframes = []

        for path in file_paths:
            if verbose:
                print(f"Loading data from {path}")
            try:
                if path.endswith('.csv'):
                    df = pd.read_csv(path)
                elif path.endswith('.pkl'):
                    df = pd.read_pickle(path)
                else:
                    df = pd.read_parquet(path)
                dataframes.append(df)
            except Exception as e:
                print(f"Error loading {path}: {e}")

        combined_df = pd.concat(dataframes, ignore_index=True)

        if verbose:
            print(f"Combined dataframe shape: {combined_df.shape}")
            print("Column names:", combined_df.columns.tolist())

        return combined_df

    def clean_data(self, df, verbose=True):
        """
        Clean the data by handling missing values, outliers, and duplicates

        Args:
            df (DataFrame): Input dataframe
            verbose (bool): Whether to print information during processing

        Returns:
            df (DataFrame): Cleaned dataframe
        """
        original_shape = df.shape

        # Check for missing values
        if verbose:
            print("Missing values before cleaning:")
            print(df.isnull().sum())

        # Apply multiple imputation techniques
        # First use forward fill for time series consistency
        df_cleaned = df.copy()
        df_cleaned.ffill(inplace=True)

        # For any remaining missing values, use backward fill
        df_cleaned.bfill(inplace=True)

        # For columns with remaining missing values, use KNN imputation
        features = ['accelerometer_uncalibrated_z', 'accelerometer_uncalibrated_y', 'accelerometer_uncalibrated_x',
                  'gyroscope_uncalibrated_z', 'gyroscope_uncalibrated_y', 'gyroscope_uncalibrated_x']

        if df_cleaned[features].isnull().any().any():
            df_cleaned[features] = pd.DataFrame(
                self.knn_imputer.fit_transform(df_cleaned[features]),
                columns=features,
                index=df_cleaned.index
            )

        # Remove duplicates
        df_cleaned = df_cleaned.drop_duplicates()

        # Handle outliers using IQR method
        for column in features:
            Q1 = df_cleaned[column].quantile(0.25)
            Q3 = df_cleaned[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Replace outliers with boundary values
            df_cleaned[column] = np.where(
                df_cleaned[column] < lower_bound,
                lower_bound,
                np.where(df_cleaned[column] > upper_bound, upper_bound, df_cleaned[column])
            )

        if verbose:
            print(f"Original shape: {original_shape}, Cleaned shape: {df_cleaned.shape}")
            print("Missing values after cleaning:")
            print(df_cleaned.isnull().sum())

        return df_cleaned

    def feature_engineering(self, df, verbose=True):
        """
        Create additional features from the sensor data

        Args:
            df (DataFrame): Input dataframe
            verbose (bool): Whether to print information during processing

        Returns:
            df (DataFrame): Dataframe with engineered features
        """
        df_engineered = df.copy()
        features = ['accelerometer_uncalibrated_z', 'accelerometer_uncalibrated_y', 'accelerometer_uncalibrated_x',
                   'gyroscope_uncalibrated_z', 'gyroscope_uncalibrated_y', 'gyroscope_uncalibrated_x']

        # Calculate magnitude of acceleration and gyroscope
        df_engineered['acc_magnitude'] = np.sqrt(
            df_engineered['accelerometer_uncalibrated_x']**2 +
            df_engineered['accelerometer_uncalibrated_y']**2 +
            df_engineered['accelerometer_uncalibrated_z']**2
        )

        df_engineered['gyro_magnitude'] = np.sqrt(
            df_engineered['gyroscope_uncalibrated_x']**2 +
            df_engineered['gyroscope_uncalibrated_y']**2 +
            df_engineered['gyroscope_uncalibrated_z']**2
        )

        # Calculate rolling statistics (with window size relative to sequence length)
        window_size = max(5, self.sequence_length // 10)

        for feature in features:
            # Rolling mean
            df_engineered[f'{feature}_rolling_mean'] = df_engineered[feature].rolling(
                window=window_size, min_periods=1).mean()

            # Rolling standard deviation
            df_engineered[f'{feature}_rolling_std'] = df_engineered[feature].rolling(
                window=window_size, min_periods=1).std().fillna(0)

        # Calculate angle features
        df_engineered['acc_xy_angle'] = np.arctan2(
            df_engineered['accelerometer_uncalibrated_y'],
            df_engineered['accelerometer_uncalibrated_x']
        )

        df_engineered['acc_yz_angle'] = np.arctan2(
            df_engineered['accelerometer_uncalibrated_z'],
            df_engineered['accelerometer_uncalibrated_y']
        )

        df_engineered['acc_xz_angle'] = np.arctan2(
            df_engineered['accelerometer_uncalibrated_z'],
            df_engineered['accelerometer_uncalibrated_x']
        )

        # Get all feature columns (excluding label)
        self.all_features = [col for col in df_engineered.columns if col != 'label']

        if verbose:
            print(f"Original features: {len(features)}, Engineered features: {len(self.all_features)}")
            print("Engineered features:", self.all_features)

        return df_engineered

    def normalize_features(self, df, verbose=True):
        """
        Apply different normalization techniques to different feature groups

        Args:
            df (DataFrame): Input dataframe with features
            verbose (bool): Whether to print information during processing

        Returns:
            df (DataFrame): Dataframe with normalized features
        """
        df_normalized = df.copy()

        # Group features by type for different scaling approaches
        basic_features = ['accelerometer_uncalibrated_z', 'accelerometer_uncalibrated_y', 'accelerometer_uncalibrated_x',
                         'gyroscope_uncalibrated_z', 'gyroscope_uncalibrated_y', 'gyroscope_uncalibrated_x']

        magnitude_features = ['acc_magnitude', 'gyro_magnitude']

        angle_features = ['acc_xy_angle', 'acc_yz_angle', 'acc_xz_angle']

        rolling_features = [col for col in self.all_features if '_rolling_' in col]

        # Apply MinMaxScaler to basic features
        if basic_features:
            df_normalized[basic_features] = self.minmax_scaler.fit_transform(df_normalized[basic_features])

        # Apply StandardScaler to magnitude features
        if all(feature in df_normalized.columns for feature in magnitude_features):
            df_normalized[magnitude_features] = self.standard_scaler.fit_transform(df_normalized[magnitude_features])

        # Angle features already normalized by nature (between -pi and pi)

        # Apply RobustScaler to rolling features
        if rolling_features:
            df_normalized[rolling_features] = self.robust_scaler.fit_transform(df_normalized[rolling_features])

        if verbose:
            print("Features normalized using different scaling techniques")

        return df_normalized

    def create_sequences(self, df, features, overlap_ratio=0.5, verbose=True):
        """
        Create sequences from time series data with optional overlap

        Args:
            df (DataFrame): Input dataframe
            features (list): List of feature columns to include
            overlap_ratio (float): Ratio of overlap between consecutive sequences
            verbose (bool): Whether to print information during processing

        Returns:
            X (ndarray): Sequence data
            y (ndarray): Labels
        """
        stride = max(1, int(self.sequence_length * (1 - overlap_ratio)))
        sequences = []
        labels = []

        # Create sequences with specified overlap
        for i in range(0, len(df) - self.sequence_length + 1, stride):
            seq = df.iloc[i:i + self.sequence_length]
            sequences.append(seq[features].values)
            # Use majority voting for the label of the sequence
            label_counts = seq['label'].value_counts()
            majority_label = label_counts.idxmax()
            labels.append(majority_label)

        X = np.array(sequences)
        y = np.array(labels)

        if verbose:
            print(f"Created {len(sequences)} sequences with shape {X.shape}")
            print(f"Label distribution: {np.bincount(y)}")

        return X, y

    def split_data(self, X, y, verbose=True):
        """
        Split the data into training, validation, and testing sets

        Args:
            X (ndarray): Sequence data
            y (ndarray): Labels
            verbose (bool): Whether to print information during processing

        Returns:
            tuple: Train, validation, and test data and labels
        """
        # Split into train+val and test
        X_train_val, X_test, y_train_val, y_test = train_test_split(
            X, y,
            test_size=self.test_size,
            random_state=self.random_state,
            stratify=y
        )

        # Split train+val into train and val
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_val, y_train_val,
            test_size=self.val_size,
            random_state=self.random_state,
            stratify=y_train_val
        )

        # Convert labels to one-hot encoding
        n_classes = len(np.unique(y))
        y_train_cat = to_categorical(y_train, num_classes=n_classes)
        y_val_cat = to_categorical(y_val, num_classes=n_classes)
        y_test_cat = to_categorical(y_test, num_classes=n_classes)

        if verbose:
            print(f"Training set: {X_train.shape}, {y_train_cat.shape}")
            print(f"Validation set: {X_val.shape}, {y_val_cat.shape}")
            print(f"Testing set: {X_test.shape}, {y_test_cat.shape}")

        return X_train, X_val, X_test, y_train_cat, y_val_cat, y_test_cat

    def preprocess(self, file_paths=None, df=None, verbose=True):
        """
        Complete preprocessing pipeline

        Args:
            file_paths (list): List of file paths to load
            df (DataFrame): Input dataframe (if already loaded)
            verbose (bool): Whether to print information during processing

        Returns:
            tuple: Train, validation, and test data and labels
        """
        if df is None and file_paths is not None:
            df = self.load_and_combine_data(file_paths, verbose)
        elif df is None and file_paths is None:
            raise ValueError("Either file_paths or df must be provided")

        # Clean the data
        df_cleaned = self.clean_data(df, verbose)

        # Feature engineering
        df_engineered = self.feature_engineering(df_cleaned, verbose)

        # Normalize features
        df_normalized = self.normalize_features(df_engineered, verbose)

        # Create sequences
        X, y = self.create_sequences(df_normalized, self.all_features, verbose=verbose)

        # Split data
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y, verbose)

        return X_train, X_val, X_test, y_train, y_val, y_test, self.all_features

    def visualize_preprocessing(self, df_original, df_processed, features=None):
        """
        Visualize the effects of preprocessing on the data

        Args:
            df_original (DataFrame): Original dataframe before preprocessing
            df_processed (DataFrame): Processed dataframe after preprocessing
            features (list): List of features to visualize
        """
        if features is None:
            features = ['accelerometer_uncalibrated_z', 'accelerometer_uncalibrated_y', 'accelerometer_uncalibrated_x',
                       'gyroscope_uncalibrated_z', 'gyroscope_uncalibrated_y', 'gyroscope_uncalibrated_x']

        # Set up the figure
        fig, axes = plt.subplots(len(features), 2, figsize=(15, 4 * len(features)))

        for i, feature in enumerate(features):
            # Original data
            sns.histplot(df_original[feature].dropna(), ax=axes[i, 0], kde=True)
            axes[i, 0].set_title(f'Original: {feature}')

            # Processed data
            sns.histplot(df_processed[feature].dropna(), ax=axes[i, 1], kde=True)
            axes[i, 1].set_title(f'Processed: {feature}')

        plt.tight_layout()
        plt.savefig('preprocessing_visualization.png')
        plt.close()

        # Create a correlation heatmap for the engineered features
        plt.figure(figsize=(12, 10))
        corr_matrix = df_processed.drop(columns=['label']).corr()
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', linewidths=0.5)
        plt.title('Feature Correlation Matrix')
        plt.tight_layout()
        plt.savefig('feature_correlation.png')
        plt.close()


# Example usage of the preprocessing class
if __name__ == "__main__":
    # Initialize the preprocessor
    preprocessor = CVNetPreprocessor(sequence_length=50, test_size=0.2, val_size=0.125)

    # Example file paths (replace with actual paths)
    file_paths = ['sensor_data1.csv', 'sensor_data2.csv']

    # Load a sample dataframe for demonstration
    # In a real scenario, you would use file_paths or provide your own dataframe
    # For demonstration, we'll create a synthetic dataset
    np.random.seed(42)
    rows = 1000

    # Generate synthetic data
    synthetic_data = {
        'accelerometer_uncalibrated_x': np.random.normal(0, 1, rows),
        'accelerometer_uncalibrated_y': np.random.normal(0, 1, rows),
        'accelerometer_uncalibrated_z': np.random.normal(0, 1, rows),
        'gyroscope_uncalibrated_x': np.random.normal(0, 0.5, rows),
        'gyroscope_uncalibrated_y': np.random.normal(0, 0.5, rows),
        'gyroscope_uncalibrated_z': np.random.normal(0, 0.5, rows),
        'label': np.random.randint(0, 2, rows)
    }

    # Create DataFrame
    df = pd.DataFrame(synthetic_data)

    # Introduce some missing values
    mask = np.random.random(df.shape) < 0.05
    df = df.mask(mask)

    # Save a copy of the original DataFrame
    df_original = df.copy()

    # Process the data
    X_train, X_val, X_test, y_train, y_val, y_test, features = preprocessor.preprocess(df=df)

    print("Preprocessing complete!")
    print(f"Number of features: {len(features)}")
    print(f"Feature names: {features}")