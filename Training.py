# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctRuxX_CdqC8cqlEZX0lOwL-GqEDQq_q
"""

import tensorflow as tf
from tensorflow.keras.callbacks import (
    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,
    TensorBoard, CSVLogger, LambdaCallback
)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from datetime import datetime
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import json
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("cv_net_training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("CV-Net Training")

class CVNetTrainer:
    """
    Trainer class for the CV-Net model with advanced training capabilities
    """
    def __init__(self, model, output_dir="cv_net_output"):
        """
        Initialize the trainer

        Args:
            model (Model): The CV-Net model to train
            output_dir (str): Directory to save outputs
        """
        self.model = model
        self.output_dir = output_dir
        self.history = None
        self.best_weights_path = None

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

    def prepare_class_weights(self, y_train):
        """
        Compute class weights for imbalanced datasets

        Args:
            y_train (ndarray): Training labels

        Returns:
            class_weights (dict): Class weights
        """
        # Convert one-hot encoded labels back to class indices
        if len(y_train.shape) > 1:
            y_indices = np.argmax(y_train, axis=1)
        else:
            y_indices = y_train

        # Compute class weights
        classes = np.unique(y_indices)
        class_weights = compute_class_weight('balanced', classes=classes, y=y_indices)

        # Create a dictionary of class weights
        class_weight_dict = dict(zip(classes, class_weights))

        logger.info(f"Computed class weights: {class_weight_dict}")

        return class_weight_dict

    def create_callbacks(self, patience=10, min_delta=0.001, save_best_only=True):
        """
        Create training callbacks

        Args:
            patience (int): Patience for early stopping
            min_delta (float): Minimum delta for early stopping
            save_best_only (bool): Whether to save only the best model

        Returns:
            callbacks (list): List of callbacks
        """
        # Create a timestamp for unique folder names
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

        # Directory for TensorBoard logs
        log_dir = os.path.join(self.output_dir, "logs", timestamp)
        os.makedirs(log_dir, exist_ok=True)

        # Directory for model checkpoints
        checkpoint_dir = os.path.join(self.output_dir, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)

        # Path for the best model weights
        self.best_weights_path = os.path.join(checkpoint_dir, f"cv_net_best_{timestamp}.weights.h5")

        # Path for CSV logs
        csv_log_path = os.path.join(self.output_dir, f"training_log_{timestamp}.csv")

        # Create callbacks
        callbacks = [
            # ModelCheckpoint to save the best model
            ModelCheckpoint(
                filepath=self.best_weights_path,
                monitor='val_accuracy',
                mode='max',
                save_best_only=save_best_only,
                save_weights_only=True,
                verbose=1
            ),

            # EarlyStopping to prevent overfitting
            EarlyStopping(
                monitor='val_loss',
                patience=patience,
                min_delta=min_delta,
                mode='min',
                restore_best_weights=True,
                verbose=1
            ),

            # ReduceLROnPlateau to adjust learning rate
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=patience // 2,
                min_lr=1e-6,
                mode='min',
                verbose=1
            ),

            # TensorBoard for visualization
            TensorBoard(
                log_dir=log_dir,
                histogram_freq=1,
                write_graph=True,
                write_images=True,
                update_freq='epoch',
                profile_batch=0,
                embeddings_freq=1
            ),

            # CSVLogger to log metrics
            CSVLogger(
                csv_log_path,
                append=True,
                separator=','
            ),

            # LambdaCallback for custom logging
            LambdaCallback(
                on_epoch_end=lambda epoch, logs: logger.info(
                    f"Epoch {epoch+1}: loss={logs.get('loss', 0):.4f}, "
                    f"accuracy={logs.get('accuracy', 0):.4f}, "
                    f"val_loss={logs.get('val_loss', 0):.4f}, "
                    f"val_accuracy={logs.get('val_accuracy', 0):.4f}, "
                    f"lr={float(tf.keras.backend.get_value(self.model.optimizer.lr)):.8f}"
                )
            )
        ]

        return callbacks

    def train(self, X_train, y_train, X_val, y_val, epochs=9, batch_size=64,
             use_class_weights=True, callbacks=None, verbose=1):
        """
        Train the CV-Net model

        Args:
            X_train (ndarray): Training data
            y_train (ndarray): Training labels
            X_val (ndarray): Validation data
            y_val (ndarray): Validation labels
            epochs (int): Number of epochs to train
            batch_size (int): Batch size
            use_class_weights (bool): Whether to use class weights
            callbacks (list): List of callbacks
            verbose (int): Verbosity level

        Returns:
            history (History): Training history
        """
        # Create callbacks if not provided
        if callbacks is None:
            callbacks = self.create_callbacks()

        # Compute class weights if needed
        class_weights = self.prepare_class_weights(y_train) if use_class_weights else None

        # Log training start
        logger.info(f"Starting training with {epochs} epochs and batch size {batch_size}")
        logger.info(f"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}")

        # Start timer
        start_time = time.time()

        # Train the model
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            class_weight=class_weights,
            verbose=verbose
        )

        # Calculate training time
        training_time = time.time() - start_time
        logger.info(f"Training completed in {training_time:.2f} seconds")

        # Store the history
        self.history = history.history

        # Save training metrics
        self.save_training_metrics()

        return history

    def save_training_metrics(self):
        """
        Save training metrics and plots
        """
        if self.history is None:
            logger.warning("No training history available to save")
            return

        # Create metrics directory
        metrics_dir = os.path.join(self.output_dir, "metrics")
        os.makedirs(metrics_dir, exist_ok=True)

        # Save history as JSON
        history_path = os.path.join(metrics_dir, "training_history.json")
        with open(history_path, 'w') as f:
            json.dump({k: list(map(float, v)) for k, v in self.history.items()}, f, indent=4)

        # Plot accuracy and loss
        plt.figure(figsize=(12, 5))

        # Plot accuracy
        plt.subplot(1, 2, 1)
        plt.plot(self.history['accuracy'], label='Training Accuracy')
        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        # Plot loss
        plt.subplot(1, 2, 2)
        plt.plot(self.history['loss'], label='Training Loss')
        plt.plot(self.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.tight_layout()
        plt.savefig(os.path.join(metrics_dir, "training_metrics.png"))
        plt.close()

        logger.info(f"Training metrics saved to {metrics_dir}")

    def evaluate(self, X_test, y_test, batch_size=None, verbose=1):
        """
        Evaluate the model on test data

        Args:
            X_test (ndarray): Test data
            y_test (ndarray): Test labels
            batch_size (int): Batch size
            verbose (int): Verbosity level

        Returns:
            metrics (dict): Evaluation metrics
        """
        # Load best weights if available
        if self.best_weights_path and os.path.exists(self.best_weights_path):
            logger.info(f"Loading best weights from {self.best_weights_path}")
            self.model.load_weights(self.best_weights_path)

        # Evaluate the model
        logger.info(f"Evaluating model on test data with shape {X_test.shape}")
        test_results = self.model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)

        # Get metric names
        metric_names = self.model.metrics_names

        # Create metrics dictionary
        metrics = {name: float(value) for name, value in zip(metric_names, test_results)}
        logger.info(f"Test metrics: {metrics}")

        # Get predictions
        y_pred_prob = self.model.predict(X_test, batch_size=batch_size, verbose=verbose)
        y_pred = np.argmax(y_pred_prob, axis=1)
        y_true = np.argmax(y_test, axis=1)

        # Generate classification report
        class_report = classification_report(y_true, y_pred, output_dict=True)
        logger.info(f"Classification report:\n{classification_report(y_true, y_pred)}")

        # Generate confusion matrix
        conf_matrix = confusion_matrix(y_true, y_pred)

        # Calculate ROC curve and AUC
        if y_test.shape[1] == 2:  # Binary classification
            fpr, tpr, _ = roc_curve(y_true, y_pred_prob[:, 1])
            roc_auc = auc(fpr, tpr)
            logger.info(f"ROC AUC: {roc_auc:.4f}")

            # ROC curve visualization
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic')
            plt.legend(loc="lower right")

            # Save ROC curve
            metrics_dir = os.path.join(self.output_dir, "metrics")
            os.makedirs(metrics_dir, exist_ok=True)
            plt.savefig(os.path.join(metrics_dir, "roc_curve.png"))
            plt.close()

            # Add AUC to metrics
            metrics['roc_auc'] = roc_auc

        # Confusion matrix visualization
        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                   xticklabels=range(y_test.shape[1]),
                   yticklabels=range(y_test.shape[1]))
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')

        # Save confusion matrix
        metrics_dir = os.path.join(self.output_dir, "metrics")
        os.makedirs(metrics_dir, exist_ok=True)
        plt.savefig(os.path.join(metrics_dir, "confusion_matrix.png"))
        plt.close()

        # Save metrics and results
        eval_results = {
            "metrics": metrics,
            "classification_report": class_report,
            "confusion_matrix": conf_matrix.tolist()
        }

        # Save evaluation results
        eval_path = os.path.join(metrics_dir, "evaluation_results.json")
        with open(eval_path, 'w') as f:
            json.dump(eval_results, f, indent=4)

        logger.info(f"Evaluation results saved to {eval_path}")

        return metrics

    def save_model_summary(self):
        """
        Save model summary and architecture
        """
        # Create directory for model information
        model_dir = os.path.join(self.output_dir, "model")
        os.makedirs(model_dir, exist_ok=True)

        # Save model summary
        summary_path = os.path.join(model_dir, "model_summary.txt")
        with open(summary_path, 'w') as f:
            # Redirect summary output to the file
            self.model.summary(print_fn=lambda x: f.write(x + '\n'))

        # Save model architecture visualization
        try:
            from tensorflow.keras.utils import plot_model
            arch_path = os.path.join(model_dir, "model_architecture.png")
            plot_model(self.model, to_file=arch_path, show_shapes=True, show_layer_names=True)
            logger.info(f"Model architecture saved to {arch_path}")
        except ImportError:
            logger.warning("Unable to save model architecture - pydot or graphviz not installed")

        # Save model configuration
        config_path = os.path.join(model_dir, "model_config.json")
        with open(config_path, 'w') as f:
            json.dump(self.model.get_config(), f, indent=4)

        logger.info(f"Model summary and configuration saved to {model_dir}")

    def visualize_attention(self, X_sample, sample_indices=None, max_samples=5):
        """
        Visualize attention weights for sample sequences

        Args:
            X_sample (ndarray): Sample input data
            sample_indices (list): Indices of samples to visualize
            max_samples (int): Maximum number of samples to visualize
        """
        # Check if attention layer exists
        attention_layer = None
        for layer in self.model.layers:
            if 'attention' in layer.name:
                attention_layer = layer
                break

        if attention_layer is None:
            logger.warning("No attention layer found in the model")
            return

        # Create a model that outputs attention weights
        attention_model = tf.keras.Model(
            inputs=self.model.inputs,
            outputs=[attention_layer.output, self.model.output]
        )

        # If no sample indices provided, select random samples
        if sample_indices is None:
            num_samples = min(max_samples, len(X_sample))
            sample_indices = np.random.choice(len(X_sample), num_samples, replace=False)

        # Create directory for attention visualizations
        attn_dir = os.path.join(self.output_dir, "attention")
        os.makedirs(attn_dir, exist_ok=True)

        # Get attention weights and predictions for samples
        X_selected = X_sample[sample_indices]
        attention_weights, predictions = attention_model.predict(X_selected)

        # Visualize attention weights for each sample
        for i, sample_idx in enumerate(sample_indices):
            plt.figure(figsize=(12, 6))

            # Plot attention weights
            plt.subplot(1, 2, 1)
            sns.heatmap(attention_weights[i], cmap='viridis')
            plt.title(f'Attention Weights - Sample {sample_idx}')
            plt.xlabel('Time Steps')
            plt.ylabel('Feature Dimension')

            # Plot feature values with attention overlay
            plt.subplot(1, 2, 2)
            seq_data = X_selected[i]
            time_steps = seq_data.shape[0]

            # Calculate aggregate attention across features
            attn_sum = np.sum(attention_weights[i], axis=1)
            attn_norm = attn_sum / np.max(attn_sum)  # Normalize

            # Plot feature values
            for j in range(min(5, seq_data.shape[1])):  # Plot up to 5 features
                plt.plot(seq_data[:, j], label=f'Feature {j}')

            # Create a second y-axis for attention
            ax2 = plt.gca().twinx()
            ax2.fill_between(range(time_steps), 0, attn_norm, alpha=0.3, color='r')
            ax2.set_ylabel('Attention', color='r')
            ax2.tick_params(axis='y', labelcolor='r')

            plt.title(f'Features with Attention - Sample {sample_idx}')
            plt.xlabel('Time Steps')
            plt.legend(loc='upper left')

            plt.tight_layout()
            plt.savefig(os.path.join(attn_dir, f'attention_sample_{sample_idx}.png'))
            plt.close()

        logger.info(f"Attention visualizations saved to {attn_dir}")


class LearningRateScheduler(tf.keras.callbacks.Callback):
    """
    Custom learning rate scheduler with warmup and decay
    """
    def __init__(self, initial_lr=0.001, min_lr=1e-6, warmup_epochs=3, decay_factor=0.8, decay_epochs=2):
        super(LearningRateScheduler, self).__init__()
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.warmup_epochs = warmup_epochs
        self.decay_factor = decay_factor
        self.decay_epochs = decay_epochs
        self.learning_rates = []

    def on_epoch_begin(self, epoch, logs=None):
        if epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.initial_lr * ((epoch + 1) / self.warmup_epochs)
        else:
            # Exponential decay after warmup
            decay_epoch = epoch - self.warmup_epochs
            if decay_epoch % self.decay_epochs == 0 and decay_epoch > 0:
                lr = max(self.min_lr, self.initial_lr * (self.decay_factor ** (decay_epoch // self.decay_epochs)))
            else:
                # Keep the same learning rate
                lr = tf.keras.backend.get_value(self.model.optimizer.lr)

        # Set the learning rate
        tf.keras.backend.set_value(self.model.optimizer.lr, lr)
        self.learning_rates.append(lr)

        print(f"\nEpoch {epoch+1}: Learning rate set to {lr}")

    def on_train_end(self, logs=None):
        # Plot learning rate schedule
        plt.figure(figsize=(10, 4))
        plt.plot(range(1, len(self.learning_rates) + 1), self.learning_rates, marker='o')
        plt.title('Learning Rate Schedule')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.yscale('log')
        plt.grid(True)
        plt.savefig('learning_rate_schedule.png')
        plt.close()


class TrainWithWarmup:
    """
    Wrapper for training with learning rate warmup and other advanced strategies
    """
    def __init__(self, model, output_dir="cv_net_output"):
        """
        Initialize the training wrapper

        Args:
            model (Model): The CV-Net model to train
            output_dir (str): Directory to save outputs
        """
        self.model = model
        self.output_dir = output_dir
        self.trainer = CVNetTrainer(model, output_dir)

    def train(self, X_train, y_train, X_val, y_val, epochs=9, batch_size=64,
             warmup_epochs=3, initial_lr=0.001, min_lr=1e-6):
        """
        Train the model with learning rate warmup

        Args:
            X_train (ndarray): Training data
            y_train (ndarray): Training labels
            X_val (ndarray): Validation data
            y_val (ndarray): Validation labels
            epochs (int): Number of epochs to train
            batch_size (int): Batch size
            warmup_epochs (int): Number of warmup epochs
            initial_lr (float): Initial learning rate
            min_lr (float): Minimum learning rate

        Returns:
            history (History): Training history
        """
        # Create standard callbacks
        callbacks = self.trainer.create_callbacks()

        # Add learning rate scheduler
        lr_scheduler = LearningRateScheduler(
            initial_lr=initial_lr,
            min_lr=min_lr,
            warmup_epochs=warmup_epochs,
            decay_factor=0.8,
            decay_epochs=3
        )
        callbacks.append(lr_scheduler)

        # Train the model
        history = self.trainer.train(
            X_train, y_train,
            X_val, y_val,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks
        )

        # Save model summary
        self.trainer.save_model_summary()

        return history


# Example usage of the training code
if __name__ == "__main__":
    from cv_net_preprocessing import CVNetPreprocessor
    from cv_net_model import CVNetModel
    import numpy as np

    # Create synthetic data for demonstration
    np.random.seed(42)
    sequence_length = 50
    num_features = 6
    num_samples = 1000

    # Create random sequences
    X = np.random.randn(num_samples, sequence_length, num_features)

    # Create random binary labels
    y = np.random.randint(0, 2, num_samples)

    # Convert to one-hot encoding
    y_onehot = tf.keras.utils.to_categorical(y, num_classes=2)

    # Split data
    from sklearn.model_selection import train_test_split

    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X, y_onehot, test_size=0.2, random_state=42
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val, test_size=0.125, random_state=42
    )

    # Create and build the model
    cv_net = CVNetModel(sequence_length, num_features, num_classes=2)
    model = cv_net.build()
    cv_net.compile_model()

    # Create trainer
    trainer = CVNetTrainer(model, output_dir="cv_net_demo_output")

    # Train the model
    history = trainer.train(
        X_train, y_train,
        X_val, y_val,
        epochs=5,  # Reduced for demonstration
        batch_size=64
    )

    # Evaluate the model
    metrics = trainer.evaluate(X_test, y_test)

    # Visualize attention for some samples
    sample_indices = np.random.choice(len(X_test), 3, replace=False)
    trainer.visualize_attention(X_test, sample_indices=sample_indices)

    print(f"Training and evaluation completed with test accuracy: {metrics.get('accuracy', 0):.4f}")

