# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctRuxX_CdqC8cqlEZX0lOwL-GqEDQq_q
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import logging
import time
from datetime import datetime
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, average_precision_score
)
import joblib
from scipy.stats import ttest_ind
from itertools import combinations
import matplotlib.cm as cm
from matplotlib.colors import Normalize
import pickle
import sys

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("cv_net_testing.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("CV-Net Testing")

class CVNetTester:
    """
    Advanced testing class for CV-Net model with comprehensive evaluation
    """
    def __init__(self, model=None, model_path=None, weights_path=None, output_dir="cv_net_testing"):
        """
        Initialize the tester

        Args:
            model (Model): The CV-Net model to test
            model_path (str): Path to load model architecture
            weights_path (str): Path to load model weights
            output_dir (str): Directory to save outputs
        """
        self.model = model
        self.output_dir = output_dir

        # Load model if provided path
        if model_path and not model:
            try:
                self.model = tf.keras.models.load_model(model_path)
                logger.info(f"Model loaded from {model_path}")
            except Exception as e:
                logger.error(f"Error loading model from {model_path}: {e}")
                raise

        # Load weights if provided path
        if weights_path and self.model:
            try:
                self.model.load_weights(weights_path)
                logger.info(f"Weights loaded from {weights_path}")
            except Exception as e:
                logger.error(f"Error loading weights from {weights_path}: {e}")
                raise

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        # Initialize results storage
        self.metrics = {}
        self.predictions = {}
        self.true_labels = {}
        self.confusion_matrices = {}

    def load_test_data(self, X_test, y_test, test_name="default"):
        """
        Load and store test data

        Args:
            X_test (ndarray): Test data
            y_test (ndarray): Test labels
            test_name (str): Name identifier for the test set

        Returns:
            X_test, y_test (tuple): Loaded test data
        """
        # Store test data
        self.predictions[test_name] = {}
        self.true_labels[test_name] = y_test

        logger.info(f"Test data '{test_name}' loaded with shape {X_test.shape}")

        return X_test, y_test

    def predict(self, X_test, test_name="default", batch_size=32, verbose=1):
        """
        Generate predictions for test data

        Args:
            X_test (ndarray): Test data
            test_name (str): Name identifier for the test set
            batch_size (int): Batch size for prediction
            verbose (int): Verbosity level

        Returns:
            y_pred (ndarray): Predicted classes
            y_prob (ndarray): Predicted probabilities
        """
        if self.model is None:
            raise ValueError("No model available for prediction")

        # Generate predictions
        logger.info(f"Generating predictions for test set '{test_name}'")
        start_time = time.time()
        y_prob = self.model.predict(X_test, batch_size=batch_size, verbose=verbose)
        prediction_time = time.time() - start_time

        # Convert probabilities to class predictions
        y_pred = np.argmax(y_prob, axis=1)

        # Store predictions
        self.predictions[test_name]['probabilities'] = y_prob
        self.predictions[test_name]['classes'] = y_pred
        self.predictions[test_name]['prediction_time'] = prediction_time

        logger.info(f"Predictions generated in {prediction_time:.2f} seconds")

        return y_pred, y_prob

    def evaluate_binary(self, y_true, y_pred, y_prob, test_name="default"):
        """
        Evaluate binary classification performance

        Args:
            y_true (ndarray): True labels
            y_pred (ndarray): Predicted classes
            y_prob (ndarray): Predicted probabilities
            test_name (str): Name identifier for the test set

        Returns:
            metrics (dict): Evaluation metrics
        """
        # Convert one-hot encoded labels to class indices if needed
        if len(y_true.shape) > 1:
            y_true_indices = np.argmax(y_true, axis=1)
        else:
            y_true_indices = y_true

        # Calculate basic metrics
        accuracy = accuracy_score(y_true_indices, y_pred)
        precision = precision_score(y_true_indices, y_pred)
        recall = recall_score(y_true_indices, y_pred)
        f1 = f1_score(y_true_indices, y_pred)

        # Calculate ROC AUC
        try:
            if y_prob.shape[1] == 2:  # Binary classification
                roc_auc = roc_auc_score(y_true_indices, y_prob[:, 1])
            else:
                roc_auc = roc_auc_score(y_true_indices, y_prob, multi_class='ovr')
        except Exception as e:
            logger.warning(f"Could not calculate ROC AUC: {e}")
            roc_auc = np.nan

        # Calculate precision-recall AUC
        try:
            if y_prob.shape[1] == 2:  # Binary classification
                precision_recall_auc = average_precision_score(y_true_indices, y_prob[:, 1])
            else:
                precision_recall_auc = average_precision_score(y_true_indices, y_prob, average='macro')
        except Exception as e:
            logger.warning(f"Could not calculate Precision-Recall AUC: {e}")
            precision_recall_auc = np.nan

        # Calculate confusion matrix
        conf_matrix = confusion_matrix(y_true_indices, y_pred)
        self.confusion_matrices[test_name] = conf_matrix

        # Store metrics
        metrics = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1),
            'roc_auc': float(roc_auc),
            'pr_auc': float(precision_recall_auc),
            'confusion_matrix': conf_matrix.tolist()
        }

        # Store in instance metrics
        self.metrics[test_name] = metrics

        # Log metrics
        logger.info(f"Evaluation results for test set '{test_name}':")
        logger.info(f"Accuracy: {accuracy:.4f}")
        logger.info(f"Precision: {precision:.4f}")
        logger.info(f"Recall: {recall:.4f}")
        logger.info(f"F1 Score: {f1:.4f}")
        logger.info(f"ROC AUC: {roc_auc:.4f}")
        logger.info(f"PR AUC: {precision_recall_auc:.4f}")

        return metrics

    def evaluate_multi(self, y_true, y_pred, y_prob, test_name="default"):
        """
        Evaluate multi-class classification performance

        Args:
            y_true (ndarray): True labels
            y_pred (ndarray): Predicted classes
            y_prob (ndarray): Predicted probabilities
            test_name (str): Name identifier for the test set

        Returns:
            metrics (dict): Evaluation metrics
        """
        # Convert one-hot encoded labels to class indices if needed
        if len(y_true.shape) > 1:
            y_true_indices = np.argmax(y_true, axis=1)
        else:
            y_true_indices = y_true

        # Calculate basic metrics
        accuracy = accuracy_score(y_true_indices, y_pred)

        # Class-wise metrics
        precision_macro = precision_score(y_true_indices, y_pred, average='macro')
        recall_macro = recall_score(y_true_indices, y_pred, average='macro')
        f1_macro = f1_score(y_true_indices, y_pred, average='macro')

        precision_weighted = precision_score(y_true_indices, y_pred, average='weighted')
        recall_weighted = recall_score(y_true_indices, y_pred, average='weighted')
        f1_weighted = f1_score(y_true_indices, y_pred, average='weighted')

        # Class-wise precision, recall, f1
        precision_per_class = precision_score(y_true_indices, y_pred, average=None)
        recall_per_class = recall_score(y_true_indices, y_pred, average=None)
        f1_per_class = f1_score(y_true_indices, y_pred, average=None)

        # Calculate ROC AUC
        try:
            roc_auc = roc_auc_score(y_true_indices, y_prob, multi_class='ovr')
        except Exception as e:
            logger.warning(f"Could not calculate ROC AUC: {e}")
            roc_auc = np.nan

        # Calculate confusion matrix
        conf_matrix = confusion_matrix(y_true_indices, y_pred)
        self.confusion_matrices[test_name] = conf_matrix

        # Store metrics
        metrics = {
            'accuracy': float(accuracy),
            'precision_macro': float(precision_macro),
            'recall_macro': float(recall_macro),
            'f1_macro': float(f1_macro),
            'precision_weighted': float(precision_weighted),
            'recall_weighted': float(recall_weighted),
            'f1_weighted': float(f1_weighted),
            'roc_auc': float(roc_auc),
            'precision_per_class': precision_per_class.tolist(),
            'recall_per_class': recall_per_class.tolist(),
            'f1_per_class': f1_per_class.tolist(),
            'confusion_matrix': conf_matrix.tolist()
        }

        # Store in instance metrics
        self.metrics[test_name] = metrics

        # Log metrics
        logger.info(f"Evaluation results for test set '{test_name}':")
        logger.info(f"Accuracy: {accuracy:.4f}")
        logger.info(f"Macro Precision: {precision_macro:.4f}")
        logger.info(f"Macro Recall: {recall_macro:.4f}")
        logger.info(f"Macro F1: {f1_macro:.4f}")
        logger.info(f"Weighted Precision: {precision_weighted:.4f}")
        logger.info(f"Weighted Recall: {recall_weighted:.4f}")
        logger.info(f"Weighted F1: {f1_weighted:.4f}")
        logger.info(f"ROC AUC (OVR): {roc_auc:.4f}")

        return metrics

    def evaluate(self, X_test, y_test, test_name="default", batch_size=32, verbose=1):
        """
        Complete evaluation pipeline

        Args:
            X_test (ndarray): Test data
            y_test (ndarray): Test labels
            test_name (str): Name identifier for the test set
            batch_size (int): Batch size for prediction
            verbose (int): Verbosity level

        Returns:
            metrics (dict): Evaluation metrics
        """
        # Load test data
        X_test, y_test = self.load_test_data(X_test, y_test, test_name)

        # Generate predictions
        y_pred, y_prob = self.predict(X_test, test_name, batch_size, verbose)

        # Determine if binary or multi-class
        num_classes = y_test.shape[1] if len(y_test.shape) > 1 else len(np.unique(y_test))

        # Evaluate based on number of classes
        if num_classes == 2:
            metrics = self.evaluate_binary(y_test, y_pred, y_prob, test_name)
        else:
            metrics = self.evaluate_multi(y_test, y_pred, y_prob, test_name)

        # Create and save visualizations
        self.visualize_results(test_name)

        return metrics

    def visualize_results(self, test_name="default"):
        """
        Create and save visualizations for evaluation results

        Args:
            test_name (str): Name identifier for the test set
        """
        if test_name not in self.metrics:
            logger.warning(f"No metrics available for test set '{test_name}'")
            return

        # Create visualizations directory
        vis_dir = os.path.join(self.output_dir, "visualizations", test_name)
        os.makedirs(vis_dir, exist_ok=True)

        # Get data
        metrics = self.metrics[test_name]
        conf_matrix = self.confusion_matrices[test_name]
        y_true = self.true_labels[test_name]
        y_true_indices = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true
        y_prob = self.predictions[test_name]['probabilities']
        y_pred = self.predictions[test_name]['classes']

        # Determine number of classes
        num_classes = y_true.shape[1] if len(y_true.shape) > 1 else len(np.unique(y_true))

        # Confusion Matrix
        plt.figure(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
                   xticklabels=range(num_classes),
                   yticklabels=range(num_classes))
        plt.xlabel("Predicted Label")
        plt.ylabel("True Label")
        plt.title(f"Confusion Matrix - {test_name}")
        plt.tight_layout()
        plt.savefig(os.path.join(vis_dir, "confusion_matrix.png"))
        plt.close()

        # ROC Curve for binary classification
        if num_classes == 2:
            plt.figure(figsize=(10, 8))

            # Calculate ROC curve
            from sklearn.metrics import roc_curve
            fpr, tpr, _ = roc_curve(y_true_indices, y_prob[:, 1])
            roc_auc = metrics.get('roc_auc', 0)

            plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], 'k--', lw=2)
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {test_name}')
            plt.legend(loc="lower right")
            plt.grid(True)
            plt.savefig(os.path.join(vis_dir, "roc_curve.png"))
            plt.close()

            # Precision-Recall Curve
            plt.figure(figsize=(10, 8))

            # Calculate Precision-Recall curve
            from sklearn.metrics import precision_recall_curve
            precision, recall, _ = precision_recall_curve(y_true_indices, y_prob[:, 1])
            pr_auc = metrics.get('pr_auc', 0)

            plt.plot(recall, precision, lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {test_name}')
            plt.legend(loc="lower left")
            plt.grid(True)
            plt.savefig(os.path.join(vis_dir, "precision_recall_curve.png"))
            plt.close()

        # Classification Report Heatmap
        if num_classes <= 10:  # Only for a reasonable number of classes
            plt.figure(figsize=(12, 8))

            # Get classification report
            report = classification_report(y_true_indices, y_pred, output_dict=True)

            # Extract class metrics
            class_metrics = {}
            for i in range(num_classes):
                class_name = str(i)
                if class_name in report:
                    class_metrics[class_name] = {
                        'precision': report[class_name]['precision'],
                        'recall': report[class_name]['recall'],
                        'f1-score': report[class_name]['f1-score'],
                        'support': report[class_name]['support']
                    }

            # Convert to DataFrame for visualization
            report_df = pd.DataFrame(class_metrics).T

            # Create heatmap
            plt.figure(figsize=(10, 8))
            sns.heatmap(report_df.iloc[:, :3], annot=True, cmap="YlGnBu", vmin=0, vmax=1)
            plt.title(f"Classification Metrics by Class - {test_name}")
            plt.tight_layout()
            plt.savefig(os.path.join(vis_dir, "classification_metrics.png"))
            plt.close()

        # Prediction Probability Distribution
        plt.figure(figsize=(12, 8))

        # For binary classification
        if num_classes == 2:
            # Class 0 probabilities
            plt.hist(y_prob[y_true_indices == 0, 1], bins=20, alpha=0.5, label='Class 0 (Actual)')
            # Class 1 probabilities
            plt.hist(y_prob[y_true_indices == 1, 1], bins=20, alpha=0.5, label='Class 1 (Actual)')
            plt.xlabel('Probability of Class 1')
        else:
            # For multi-class, plot distribution of highest probability
            max_probs = np.max(y_prob, axis=1)
            plt.hist(max_probs, bins=20)
            plt.xlabel('Maximum Class Probability')

        plt.ylabel('Count')
        plt.title(f'Prediction Probability Distribution - {test_name}')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(vis_dir, "probability_distribution.png"))
        plt.close()

        # Misclassification Analysis
        misclassified = y_pred != y_true_indices
        num_misclassified = np.sum(misclassified)

        if num_misclassified > 0:
            # Create table of misclassified instances
            misclassified_data = {
                'True Class': y_true_indices[misclassified],
                'Predicted Class': y_pred[misclassified],
                'Confidence': np.max(y_prob[misclassified], axis=1)
            }
            misclassified_df = pd.DataFrame(misclassified_data)

            # Save to CSV
            misclassified_df.to_csv(os.path.join(vis_dir, "misclassified.csv"), index=False)

            # Plot most common misclassifications
            if num_classes <= 10:
                plt.figure(figsize=(12, 8))

                # Create matrix of misclassifications (true vs predicted)
                misclass_matrix = np.zeros((num_classes, num_classes))
                for i, j in zip(y_true_indices[misclassified], y_pred[misclassified]):
                    misclass_matrix[i, j] += 1

                # Set diagonal to zero (correctly classified)
                np.fill_diagonal(misclass_matrix, 0)

                # Plot heatmap
                sns.heatmap(misclass_matrix, annot=True, fmt="d", cmap="Reds",
                           xticklabels=range(num_classes),
                           yticklabels=range(num_classes))
                plt.xlabel("Predicted Class")
                plt.ylabel("True Class")
                plt.title(f"Misclassification Matrix - {test_name}")
                plt.tight_layout()
                plt.savefig(os.path.join(vis_dir, "misclassification_matrix.png"))
                plt.close()

        logger.info(f"Visualizations saved to {vis_dir}")

    def save_results(self):
        """
        Save all evaluation results to files
        """
        # Create results directory
        results_dir = os.path.join(self.output_dir, "results")
        os.makedirs(results_dir, exist_ok=True)

        # Save metrics
        metrics_path = os.path.join(results_dir, "metrics.json")
        with open(metrics_path, 'w') as f:
            json.dump(self.metrics, f, indent=4)

        # Save confusion matrices
        for test_name, conf_matrix in self.confusion_matrices.items():
            conf_matrix_path = os.path.join(results_dir, f"confusion_matrix_{test_name}.csv")
            np.savetxt(conf_matrix_path, conf_matrix, delimiter=',', fmt='%d')

        # Create summary report
        report = {
            'test_sets': list(self.metrics.keys()),
            'num_test_sets': len(self.metrics),
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'summary': {}
        }

        # Add summary metrics for each test set
        for test_name, metrics in self.metrics.items():
            report['summary'][test_name] = {
                'accuracy': metrics.get('accuracy', 0),
                'precision': metrics.get('precision', metrics.get('precision_macro', 0)),
                'recall': metrics.get('recall', metrics.get('recall_macro', 0)),
                'f1': metrics.get('f1', metrics.get('f1_macro', 0)),
                'roc_auc': metrics.get('roc_auc', 0)
            }

        # Save summary report
        report_path = os.path.join(results_dir, "summary_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=4)

        logger.info(f"All results saved to {results_dir}")

        return report

    def analyze_errors(self, X_test, test_name="default", max_samples=10):
        """
        Analyze prediction errors in more detail

        Args:
            X_test (ndarray): Test data
            test_name (str): Name identifier for the test set
            max_samples (int): Maximum number of samples to analyze

        Returns:
            error_analysis (dict): Error analysis results
        """
        if test_name not in self.predictions:
            logger.warning(f"No predictions available for test set '{test_name}'")
            return {}

        # Get predictions and true labels
        y_pred = self.predictions[test_name]['classes']
        y_prob = self.predictions[test_name]['probabilities']
        y_true = self.true_labels[test_name]
        y_true_indices = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true

        # Find misclassified samples
        misclassified = y_pred != y_true_indices
        misclassified_indices = np.where(misclassified)[0]

        if len(misclassified_indices) == 0:
            logger.info(f"No misclassifications found in test set '{test_name}'")
            return {'num_errors': 0}

        # Limit the number of samples to analyze
        sample_indices = misclassified_indices[:min(max_samples, len(misclassified_indices))]

        # Analyze misclassified samples
        error_analysis = {
            'num_errors': int(np.sum(misclassified)),
            'error_rate': float(np.mean(misclassified)),
            'samples': []
        }

        # Create error analysis for each sample
        for idx in sample_indices:
            true_class = int(y_true_indices[idx])
            pred_class = int(y_pred[idx])
            confidence = float(y_prob[idx][pred_class])
            true_confidence = float(y_prob[idx][true_class])

            sample_analysis = {
                'index': int(idx),
                'true_class': true_class,
                'predicted_class': pred_class,
                'confidence': confidence,
                'true_class_confidence': true_confidence,
                'confidence_diff': float(confidence - true_confidence)
            }

            error_analysis['samples'].append(sample_analysis)

        # Create results directory
        results_dir = os.path.join(self.output_dir, "results")
        os.makedirs(results_dir, exist_ok=True)

        # Save error analysis
        error_path = os.path.join(results_dir, f"error_analysis_{test_name}.json")
        with open(error_path, 'w') as f:
            json.dump(error_analysis, f, indent=4)

        logger.info(f"Error analysis saved to {error_path}")

        return error_analysis

    def extract_feature_importance(self, X_test, test_name="default", num_permutations=10):
        """
        Estimate feature importance using permutation method

        Args:
            X_test (ndarray): Test data
            test_name (str): Name identifier for the test set
            num_permutations (int): Number of permutations for each feature

        Returns:
            feature_importance (dict): Feature importance scores
        """
        if test_name not in self.true_labels:
            logger.warning(f"No labels available for test set '{test_name}'")
            return {}

        # Get true labels
        y_true = self.true_labels[test_name]
        y_true_indices = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true

        # Get baseline accuracy
        if test_name not in self.metrics:
            # Make predictions to calculate baseline metrics
            self.predict(X_test, test_name)
            y_pred = self.predictions[test_name]['classes']
            baseline_accuracy = accuracy_score(y_true_indices, y_pred)
        else:
            baseline_accuracy = self.metrics[test_name].get('accuracy', 0)

        # Number of features (in the last dimension of X_test)
        num_features = X_test.shape[2]

        # Store feature importance
        feature_importance = {
            'baseline_accuracy': float(baseline_accuracy),
            'features': []
        }

        logger.info(f"Extracting feature importance for {num_features} features with {num_permutations} permutations each")

        # For each feature
        for feature_idx in range(num_features):
            logger.info(f"Processing feature {feature_idx + 1}/{num_features}")

            # Store permutation results
            permutation_scores = []

            # Repeat permutation multiple times
            for i in range(num_permutations):
                # Copy the test data
                X_permuted = X_test.copy()

                # Permute the feature across all sequences
                permuted_values = X_permuted[:, :, feature_idx].flatten()
                np.random.shuffle(permuted_values)
                X_permuted[:, :, feature_idx] = permuted_values.reshape(X_permuted.shape[0], X_permuted.shape[1])

                # Make predictions with permuted data
                y_perm_pred = self.model.predict(X_permuted)
                y_perm_pred_classes = np.argmax(y_perm_pred, axis=1)

                # Calculate accuracy with permuted feature
                permuted_accuracy = accuracy_score(y_true_indices, y_perm_pred_classes)

                # Store the accuracy drop
                accuracy_drop = baseline_accuracy - permuted_accuracy
                permutation_scores.append(accuracy_drop)

            # Calculate mean and std of permutation scores
            mean_importance = float(np.mean(permutation_scores))
            std_importance = float(np.std(permutation_scores))

            # Store feature importance
            feature_importance['features'].append({
                'index': feature_idx,
                'importance': mean_importance,
                'std': std_importance
            })

            logger.info(f"Feature {feature_idx} importance: {mean_importance:.4f} (Â±{std_importance:.4f})")

        # Sort features by importance
        feature_importance['features'].sort(key=lambda x: x['importance'], reverse=True)

        # Create results directory
        results_dir = os.path.join(self.output_dir, "results")
        os.makedirs(results_dir, exist_ok=True)

        # Save feature importance
        importance_path = os.path.join(results_dir, f"feature_importance_{test_name}.json")
        with open(importance_path, 'w') as f:
            json.dump(feature_importance, f, indent=4)

        # Visualize feature importance
        vis_dir = os.path.join(self.output_dir, "visualizations", test_name)
        os.makedirs(vis_dir, exist_ok=True)

        plt.figure(figsize=(12, 8))

        # Extract data for plotting
        indices = [f['index'] for f in feature_importance['features']]
        importances = [f['importance'] for f in feature_importance['features']]
        stds = [f['std'] for f in feature_importance['features']]

        # Plot feature importance
        plt.bar(range(len(indices)), importances, yerr=stds, align='center')
        plt.xticks(range(len(indices)), indices)
        plt.xlabel('Feature Index')
        plt.ylabel('Importance (Accuracy Drop)')
        plt.title(f'Feature Importance - {test_name}')
        plt.tight_layout()
        plt.savefig(os.path.join(vis_dir, "feature_importance.png"))
        plt.close()

        logger.info(f"Feature importance analysis saved to {importance_path}")

        return feature_importance

    def analyze_attention_patterns(self, X_sample, sample_indices=None, max_samples=5, test_name="default"):
        """
        Analyze attention patterns for sample sequences

        Args:
            X_sample (ndarray): Sample input data
            sample_indices (list): Indices of samples to analyze
            max_samples (int): Maximum number of samples to analyze
            test_name (str): Name identifier for the test set

        Returns:
            attention_analysis (dict): Attention pattern analysis
        """
        # Check if attention layer exists
        attention_layer = None
        for layer in self.model.layers:
            if 'attention' in layer.name:
                attention_layer = layer
                break

        if attention_layer is None:
            logger.warning("No attention layer found in the model")
            return {}

        # Create a model that outputs attention weights
        try:
            attention_model = tf.keras.Model(
                inputs=self.model.inputs,
                outputs=[attention_layer.output, self.model.output]
            )
        except:
            logger.warning("Could not create attention model")
            return {}

        # If no sample indices provided, select random samples
        if sample_indices is None:
            num_samples = min(max_samples, len(X_sample))
            sample_indices = np.random.choice(len(X_sample), num_samples, replace=False)

        # Get predictions and attention weights for samples
        X_selected = X_sample[sample_indices]

        try:
            attention_weights, predictions = attention_model.predict(X_selected)
        except:
            logger.warning("Could not get attention weights from model")
            return {}

        # Create visualizations directory
        vis_dir = os.path.join(self.output_dir, "visualizations", test_name, "attention")
        os.makedirs(vis_dir, exist_ok=True)

        # Store attention analysis
        attention_analysis = {
            'num_samples': len(sample_indices),
            'samples': []
        }

        # Analyze each sample
        for i, sample_idx in enumerate(sample_indices):
            # Visualize attention weights
            plt.figure(figsize=(12, 6))

            # Plot attention heatmap
            plt.subplot(1, 2, 1)
            sns.heatmap(attention_weights[i], cmap='viridis')
            plt.title(f'Attention Weights - Sample {sample_idx}')
            plt.xlabel('Time Steps')
            plt.ylabel('Features')

            # Calculate attention summary statistics
            mean_attention = np.mean(attention_weights[i], axis=1)

            # Plot mean attention over time
            plt.subplot(1, 2, 2)
            plt.plot(mean_attention)
            plt.title(f'Mean Attention Over Time - Sample {sample_idx}')
            plt.xlabel('Time Steps')
            plt.ylabel('Mean Attention')
            plt.grid(True)

            plt.tight_layout()
            plt.savefig(os.path.join(vis_dir, f'attention_sample_{sample_idx}.png'))
            plt.close()

            # Find time steps with highest attention
            top_timesteps = np.argsort(-mean_attention)[:5]  # Top 5 time steps

            # Store sample analysis
            sample_analysis = {
                'index': int(sample_idx),
                'mean_attention': float(np.mean(attention_weights[i])),
                'max_attention': float(np.max(attention_weights[i])),
                'top_timesteps': top_timesteps.tolist(),
                'attention_per_timestep': mean_attention.tolist()
            }

            # Add prediction info if available
            if test_name in self.true_labels:
                y_true = self.true_labels[test_name]
                y_true_indices = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true
                true_class = int(y_true_indices[sample_idx])
                pred_class = int(np.argmax(predictions[i]))

                sample_analysis['true_class'] = true_class
                sample_analysis['predicted_class'] = pred_class
                sample_analysis['correct_prediction'] = true_class == pred_class

            attention_analysis['samples'].append(sample_analysis)

        # Create results directory
        results_dir = os.path.join(self.output_dir, "results")
        os.makedirs(results_dir, exist_ok=True)

        # Save attention analysis
        attention_path = os.path.join(results_dir, f"attention_analysis_{test_name}.json")
        with open(attention_path, 'w') as f:
            json.dump(attention_analysis, f, indent=4)

        logger.info(f"Attention analysis saved to {attention_path}")

        return attention_analysis

    def run_complete_evaluation(self, X_test, y_test, test_name="default", feature_importance=False):
        """
        Run a complete evaluation including metrics, visualizations, and feature analysis

        Args:
            X_test (ndarray): Test data
            y_test (ndarray): Test labels
            test_name (str): Name identifier for the test set
            feature_importance (bool): Whether to calculate feature importance

        Returns:
            results (dict): Complete evaluation results
        """
        logger.info(f"Starting complete evaluation for test set '{test_name}'")

        # Core evaluation
        metrics = self.evaluate(X_test, y_test, test_name=test_name)

        # Error analysis
        error_analysis = self.analyze_errors(X_test, test_name=test_name)

        # Feature importance (optional, can be time-consuming)
        importance_results = {}
        if feature_importance:
            importance_results = self.extract_feature_importance(X_test, test_name=test_name)

        # Attention analysis (on a small subset)
        num_samples = min(10, len(X_test))
        sample_indices = np.random.choice(len(X_test), num_samples, replace=False)
        attention_analysis = self.analyze_attention_patterns(X_test, sample_indices, test_name=test_name)

        # Save all results
        results = self.save_results()

        # Add additional analyses to results
        results['error_analysis'] = error_analysis
        if feature_importance:
            results['feature_importance'] = importance_results
        results['attention_analysis'] = attention_analysis

        # Save complete results
        complete_results_path = os.path.join(self.output_dir, "complete_evaluation.json")
        with open(complete_results_path, 'w') as f:
            json.dump(results, f, indent=4)

        logger.info(f"Complete evaluation results saved to {complete_results_path}")

        return results


# Example usage of the testing code
if __name__ == "__main__":
    from cv_net_preprocessing import CVNetPreprocessor
    from cv_net_model import CVNetModel
    import numpy as np

    # Create synthetic data for demonstration
    np.random.seed(42)
    sequence_length = 50
    num_features = 6
    num_samples = 1000

    # Create random sequences
    X = np.random.randn(num_samples, sequence_length, num_features)

    # Create random binary labels
    y = np.random.randint(0, 2, num_samples)

    # Convert to one-hot encoding
    y_onehot = tf.keras.utils.to_categorical(y, num_classes=2)

    # Split data
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_onehot, test_size=0.2, random_state=42
    )

    # Create and build the model
    cv_net = CVNetModel(sequence_length, num_features, num_classes=2)
    model = cv_net.build()
    cv_net.compile_model()

    # Train the model (minimal training just for demonstration)
    model.fit(X_train, y_train, epochs=2, batch_size=64, verbose=1)

    # Create tester
    tester = CVNetTester(model, output_dir="cv_net_test_output")

    # Run evaluation
    metrics = tester.evaluate(X_test, y_test)

    # Optional: Run complete evaluation
    # results = tester.run_complete_evaluation(X_test, y_test, feature_importance=False)

    print(f"Testing completed with accuracy: {metrics.get('accuracy', 0):.4f}")

