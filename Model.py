# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ctRuxX_CdqC8cqlEZX0lOwL-GqEDQq_q
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout, BatchNormalization, Flatten,
    Multiply, Concatenate, Attention as AdditiveAttention
)

class CVNetModel:
    """
    CV-Net model implementation with attention mechanism for sensor data classification
    """
    def __init__(self, sequence_length, num_features, num_classes=2):
        """
        Initialize the CV-Net model

        Args:
            sequence_length (int): Length of input sequences
            num_features (int): Number of features per time step
            num_classes (int): Number of output classes
        """
        self.sequence_length = sequence_length
        self.num_features = num_features
        self.num_classes = num_classes
        self.model = None

    def build(self):
        """
        Build the CV-Net model architecture

        Returns:
            model (Model): Built CV-Net model
        """
        # Define the input layer
        input_layer = Input(shape=(self.sequence_length, self.num_features))

        # Adding LSTM layers
        x = LSTM(units=64, return_sequences=True)(input_layer)
        x = Dropout(0.3)(x)
        x = LSTM(units=64, return_sequences=True)(x)
        x = Dropout(0.3)(x)

        # Adding self-attention mechanism
        attention = AdditiveAttention(name='attention_weight')
        attention_output = attention([x, x])

        # Multiplying attention output with LSTM output
        x = Multiply()([x, attention_output])

        # Flatten the output for the dense layer
        x = Flatten()(x)

        # Adding a dense layer
        x = Dense(64, activation='relu')(x)

        # Adding Batch Normalization and Dropout
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)

        # Output layer
        output_layer = Dense(self.num_classes, activation='softmax')(x)

        # Create the model
        self.model = Model(inputs=input_layer, outputs=output_layer)

        return self.model

    def compile_model(self, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):
        """
        Compile the CV-Net model

        Args:
            optimizer (str or object): Optimizer for model training
            loss (str): Loss function
            metrics (list): Metrics to monitor during training

        Returns:
            model (Model): Compiled model
        """
        if self.model is None:
            self.build()

        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
        return self.model

    def summary(self):
        """
        Print the model summary
        """
        if self.model is None:
            self.build()

        return self.model.summary()

    def save_model(self, filepath):
        """
        Save the model to a file

        Args:
            filepath (str): Path to save the model
        """
        if self.model is None:
            raise ValueError("Model has not been built yet.")

        self.model.save(filepath)
        print(f"Model saved to {filepath}")

    def save_weights(self, filepath):
        """
        Save the model weights to a file

        Args:
            filepath (str): Path to save the model weights
        """
        if self.model is None:
            raise ValueError("Model has not been built yet.")

        self.model.save_weights(filepath)
        print(f"Model weights saved to {filepath}")

    def load_weights(self, filepath):
        """
        Load model weights from a file

        Args:
            filepath (str): Path to load the model weights from
        """
        if self.model is None:
            self.build()

        self.model.load_weights(filepath)
        print(f"Model weights loaded from {filepath}")

    def get_attention_layer(self):
        """
        Get the attention layer from the model

        Returns:
            layer: The attention layer
        """
        if self.model is None:
            raise ValueError("Model has not been built yet.")

        for layer in self.model.layers:
            if 'attention' in layer.name:
                return layer

        return None

    def plot_model(self, filepath='cv_net_model.png', show_shapes=True):
        """
        Plot the model architecture

        Args:
            filepath (str): Path to save the model plot
            show_shapes (bool): Whether to show layer shapes in the plot
        """
        if self.model is None:
            self.build()

        try:
            from tensorflow.keras.utils import plot_model
            plot_model(self.model, to_file=filepath, show_shapes=show_shapes)
            print(f"Model plot saved to {filepath}")
        except ImportError:
            print("Cannot plot model - pydot or graphviz not installed")


class EnhancedCVNetModel(CVNetModel):
    """
    Enhanced CV-Net model with additional options and configurations
    """
    def __init__(self, sequence_length, num_features, num_classes=2):
        super().__init__(sequence_length, num_features, num_classes)

    def build_enhanced(self, lstm_units=[64, 64], dropout_rate=0.3, dense_units=64,
                      attention_activation='tanh', dense_activation='relu'):
        """
        Build an enhanced version of the CV-Net model

        Args:
            lstm_units (list): Units for each LSTM layer
            dropout_rate (float): Dropout rate
            dense_units (int): Units for dense layer
            attention_activation (str): Activation for attention layer
            dense_activation (str): Activation for dense layer

        Returns:
            model (Model): Built enhanced model
        """
        # Define the input layer
        input_layer = Input(shape=(self.sequence_length, self.num_features))

        # Adding LSTM layers with configurable units
        x = LSTM(units=lstm_units[0], return_sequences=True,
                name='lstm_1',
                kernel_initializer='glorot_uniform',
                recurrent_initializer='orthogonal')(input_layer)
        x = Dropout(dropout_rate, name='dropout_1')(x)

        x = LSTM(units=lstm_units[1], return_sequences=True,
                name='lstm_2',
                kernel_initializer='glorot_uniform',
                recurrent_initializer='orthogonal')(x)
        x = Dropout(dropout_rate, name='dropout_2')(x)

        # Adding self-attention mechanism with configurable activation
        attention = AdditiveAttention(use_scale=True, name='attention_weight')
        attention_output = attention([x, x])

        # Multiplying attention output with LSTM output
        x = Multiply(name='attention_multiply')([x, attention_output])

        # Flatten the output for the dense layer
        x = Flatten(name='flatten')(x)

        # Adding a dense layer with configurable units and activation
        x = Dense(dense_units, activation=dense_activation, name='dense_1',
                 kernel_initializer='he_normal')(x)

        # Adding Batch Normalization and Dropout
        x = BatchNormalization(name='batch_norm')(x)
        x = Dropout(dropout_rate, name='dropout_3')(x)

        # Output layer
        output_layer = Dense(self.num_classes, activation='softmax', name='output',
                            kernel_initializer='glorot_uniform')(x)

        # Create the model
        self.model = Model(inputs=input_layer, outputs=output_layer, name='CV-Net')

        return self.model

    def analyze_attention(self, X_sample):
        """
        Analyze the attention weights for a sample input

        Args:
            X_sample (ndarray): Sample input data

        Returns:
            attention_weights (ndarray): Attention weights
        """
        if self.model is None:
            raise ValueError("Model has not been built yet.")

        # Create a new model that outputs attention weights
        attention_layer = self.get_attention_layer()

        if attention_layer is None:
            raise ValueError("No attention layer found in the model.")

        # Create a model that outputs attention weights
        attention_model = Model(inputs=self.model.input,
                              outputs=attention_layer.output)

        # Get attention weights
        attention_weights = attention_model.predict(X_sample)

        return attention_weights


# Example usage of the model class
if __name__ == "__main__":
    # Example parameters
    sequence_length = 50
    num_features = 6  # For the basic 6 sensor features
    num_classes = 2

    # Create a basic CV-Net model
    cv_net = CVNetModel(sequence_length, num_features, num_classes)
    model = cv_net.build()
    cv_net.compile_model()
    cv_net.summary()

    # Create an enhanced CV-Net model
    enhanced_cv_net = EnhancedCVNetModel(sequence_length, num_features, num_classes)
    enhanced_model = enhanced_cv_net.build_enhanced(
        lstm_units=[128, 128],
        dropout_rate=0.4,
        dense_units=128,
        dense_activation='relu'
    )
    enhanced_cv_net.compile_model()
    enhanced_cv_net.summary()

    print("CV-Net models created successfully!")